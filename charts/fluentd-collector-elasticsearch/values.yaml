# Default values for fluentd.
# This is a YAML-formatted file.
# Declare variables to be passed into your templates.
image:
  repository: gcr.io/google-containers/fluentd-elasticsearch
  tag: v2.4.0
  pullPolicy: IfNotPresent
  # pullSecrets:
  #   - secret1
  #   - secret2


output:
  host: elasticsearch-client.default.svc.cluster.local
  port: 9200
  scheme: https
  sslVersion: TLSv1_2
  buffer_chunk_limit: 2M
  buffer_queue_limit: 8

env: {}

# Extra Environment Values - allows yaml definitions
# extraEnvVars: {}

extraVolumes:
  - name: es-certs
    secret:
      defaultMode: 420
      secretName: ""
extraVolumeMounts:
  - name: es-certs
    mountPath: /certs
    readOnly: true
  
plugins:
  enabled: true
  pluginsList: ["fluent-plugin-beats"]

service:
  annotations: {}
  type: ClusterIP
  # loadBalancerIP:
  # type: NodePort
  # nodePort:
  # Used to create Service records
  ports:
    - name: "monitor-agent"
      protocol: TCP
      containerPort: 24220
    - name: "beats"
      protocol: TCP
      containerPort: 5044
    - name: "http"
      protocol: TCP
      containerPort: 5080
    - name: "syslog-tcp"
      protocol: TCP
      containerPort: 5140
    - name: "syslog-udp"
      protocol: UDP
      containerPort: 5140


metrics:
  enabled: false
  service:
    port: 24231
  serviceMonitor:
    enabled: false
    additionalLabels: {}
    # namespace: monitoring
    # interval: 30s
    # scrapeTimeout: 10s

annotations: {}
#  prometheus.io/scrape: "true"
#  prometheus.io/port: "24231"


configMaps:
  general.conf: |
    # Prevent fluentd from handling records containing its own logs. Otherwise
    # it can lead to an infinite loop, when error in sending one message generates
    # another message which also fails to be sent and so on.
    <match fluentd.**>
      @type null
    </match>

    # Used for health checking
    <source>
      @type http
      port 9880
      bind 0.0.0.0
    </source>

    # Emits internal metrics to every minute, and also exposes them on port
    # 24220. Useful for determining if an output plugin is retryring/erroring,
    # or determining the buffer queue length.
    <source>
      @type monitor_agent
      bind 0.0.0.0
      port 24220
      tag fluentd.monitor.metrics
    </source>
  system.conf: |-
    <system>
      root_dir /tmp/fluentd-buffers/
    </system>
  forward-input.conf: |
    <source>
      @type forward
      port 24224
      bind 0.0.0.0
    </source>
  http-input.conf: |
   <source>
      @type http
      port 5080
      bind 0.0.0.0
    </source>
  beats-input.conf: |
    <source>
      @type beats
      metadata_as_tag
      port 5044
      bind 0.0.0.0
    </source>
  syslog-tcp-input.conf: |
    <source>
      @type syslog
      port 5140
      bind 0.0.0.0
      <transport tcp>
      </transport>
      tag syslog-tcp
    </source>
  syslog-udp-input.conf: |
    <source>
      @type syslog
      port 5140
      bind 0.0.0.0
      tag syslog-udp
    </source>
  output.conf: |
    <match **>
      @id elasticsearch
      @type elasticsearch
      @log_level info
      include_tag_key true
      # Replace with the host/port to your Elasticsearch cluster.
      host "#{ENV['OUTPUT_HOST']}"
      port "#{ENV['OUTPUT_PORT']}"
      scheme "#{ENV['OUTPUT_SCHEME']}"
      ssl_version "#{ENV['OUTPUT_SSL_VERSION']}"
      ssl_verify true
      ca_file /certs/ca.crt
      user "#{ENV['ES_USERNAME']}"
      password "#{ENV['ES_PASSWORD']}"

      logstash_format true
      logstash_prefix ${tag}

      # Solves log gaps in K8s
      # https://github.com/uken/fluent-plugin-elasticsearch/blob/master/README.Troubleshooting.md#stopped-to-send-events-on-k8s-why
      reload_connections false
      reconnect_on_error true
      reload_on_failure true

      <buffer>
        @type file
        path /var/log/fluentd-buffers/kubernetes.system.buffer
        flush_mode interval
        retry_type exponential_backoff
        flush_thread_count 2
        flush_interval 5s
        retry_forever
        retry_max_interval 30
        chunk_limit_size "#{ENV['OUTPUT_BUFFER_CHUNK_LIMIT']}"
        queue_limit_length "#{ENV['OUTPUT_BUFFER_QUEUE_LIMIT']}"
        overflow_action block
      </buffer>
    </match>

resources: {}
  # We usually recommend not to specify default resources and to leave this as a conscious
  # choice for the user. This also increases chances charts run on environments with little
  # resources, such as Minikube. If you do want to specify resources, uncomment the following
  # lines, adjust them as necessary, and remove the curly braces after 'resources:'.
  # limits:
  #  cpu: 500m
  #  memory: 200Mi
  # requests:
  #  cpu: 500m
  #  memory: 200Mi

rbac:
  # Specifies whether RBAC resources should be created
  create: true

serviceAccount:
  # Specifies whether a ServiceAccount should be created
  create: true

## Persist data to a persistent volume
persistence:
  enabled: false

  ## If defined, storageClassName: <storageClass>
  ## If set to "-", storageClassName: "", which disables dynamic provisioning
  ## If undefined (the default) or set to null, no storageClassName spec is
  ##   set, choosing the default provisioner.  (gp2 on AWS, standard on
  ##   GKE, AWS & OpenStack)
  ##
  # storageClass: "-"
  # annotations: {}
  accessMode: ReadWriteOnce
  size: 10Gi

nodeSelector: {}

tolerations: []

affinity: {}

# Enable autoscaling using HorizontalPodAutoscaler
autoscaling:
  enabled: false
  minReplicas: 1
  maxReplicas: 5
  metrics:
    - type: Resource
      resource:
        name: cpu
        target:
          type: Utilization
          averageUtilization: 60
    - type: Resource
      resource:
        name: memory
        target:
          type: Utilization
          averageUtilization: 60

# Consider to set higher value when using in conjuction with autoscaling
# Full description about this field: https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.15/#pod-v1-core
terminationGracePeriodSeconds: 30

